{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHAamXF-Ba0t",
        "outputId": "dee7b9a4-100a-44fc-dbed-42dffa5fa57d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7ytSlkcOKAR",
        "outputId": "d3111dab-aa8a-42a4-b732-095baa7aab7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.18.0\n",
            "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
            "Wed May 28 19:31:50 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P8              9W /   70W |       2MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
        "\n",
        "# Check GPU name\n",
        "!nvidia-smi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxzlXMoQHSyz",
        "outputId": "7dc0bbd5-53ab-49e5-dee7-2215a12beb36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (3.13.0)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.11/dist-packages (2.37.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.11/dist-packages (0.25.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.11/dist-packages (from h5py) (2.0.2)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.11/dist-packages (from imageio) (11.2.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (1.15.3)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (3.4.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (2025.5.21)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image) (0.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install h5py imageio matplotlib scikit-image tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PbazQOLMZNt",
        "outputId": "e5b3f958-6bbd-4c75-97e6-868993b3ab42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following packages were automatically installed and are no longer required:\n",
            "  default-libmysqlclient-dev libaom-dev libarmadillo-dev libarpack2-dev\n",
            "  libblosc-dev libcfitsio-dev libdav1d-dev libde265-dev libfreexl-dev\n",
            "  libfyba-dev libgeos-dev libgeotiff-dev libgif-dev libheif-dev libjson-c-dev\n",
            "  libkml-dev libkmlconvenience1 libkmlregionator1 libkmlxsd1 liblz4-dev\n",
            "  libminizip-dev libmysqlclient-dev libnetcdf-dev libodbccr2 libogdi-dev\n",
            "  libopenjp2-7-dev libpoppler-dev libpoppler-private-dev libpq-dev libproj-dev\n",
            "  libqhull-dev libqhull8.0 libqhullcpp8.0 librttopo-dev libspatialite-dev\n",
            "  libsqlite3-dev libsuperlu-dev liburiparser-dev libwebp-dev libx265-dev\n",
            "  libxerces-c-dev unixodbc-dev\n",
            "Use 'apt autoremove' to remove them.\n",
            "Suggested packages:\n",
            "  libhdf4-doc hdf4-tools\n",
            "The following packages will be REMOVED:\n",
            "  libgdal-dev libhdf4-alt-dev\n",
            "The following NEW packages will be installed:\n",
            "  libhdf4-0 libhdf4-dev\n",
            "0 upgraded, 2 newly installed, 2 to remove and 35 not upgraded.\n",
            "Need to get 792 kB of archives.\n",
            "After this operation, 573 kB disk space will be freed.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libhdf4-0 amd64 4.2.15-4 [334 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libhdf4-dev amd64 4.2.15-4 [458 kB]\n",
            "Fetched 792 kB in 2s (350 kB/s)\n",
            "(Reading database ... 126109 files and directories currently installed.)\n",
            "Removing libgdal-dev (3.8.4+dfsg-1~jammy0) ...\n",
            "Removing libhdf4-alt-dev (4.2.15-4) ...\n",
            "Selecting previously unselected package libhdf4-0.\n",
            "(Reading database ... 125961 files and directories currently installed.)\n",
            "Preparing to unpack .../libhdf4-0_4.2.15-4_amd64.deb ...\n",
            "Unpacking libhdf4-0 (4.2.15-4) ...\n",
            "Selecting previously unselected package libhdf4-dev.\n",
            "Preparing to unpack .../libhdf4-dev_4.2.15-4_amd64.deb ...\n",
            "Unpacking libhdf4-dev (4.2.15-4) ...\n",
            "Setting up libhdf4-0 (4.2.15-4) ...\n",
            "Setting up libhdf4-dev (4.2.15-4) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "Collecting pyhdf\n",
            "  Downloading pyhdf-0.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pyhdf) (2.0.2)\n",
            "Downloading pyhdf-0.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (780 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m780.3/780.3 kB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyhdf\n",
            "Successfully installed pyhdf-0.11.6\n"
          ]
        }
      ],
      "source": [
        "!apt-get install libhdf4-0 libhdf4-dev -y\n",
        "!pip install pyhdf\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7ixKelmHqeI",
        "outputId": "9b6017be-1b91-43b8-fc6c-5df2e5410fbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define paths\n",
        "input_dir = \"/content/drive/MyDrive/MOD09GQ_061-20250521_065631\" # change if your folder name is different\n",
        "output_hr_dir = \"/content/drive/MyDrive/SRGAN_data/HR\"\n",
        "output_lr_dir = \"/content/drive/MyDrive/SRGAN_data/LR\"\n",
        "\n",
        "import os\n",
        "os.makedirs(output_hr_dir, exist_ok=True)\n",
        "os.makedirs(output_lr_dir, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NCYCxBPH5kJ",
        "outputId": "625cd4fe-ceea-4eb0-c3fd-36293268b5d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "🔄 Processing HDF files: 100%|██████████| 100/100 [04:42<00:00,  2.82s/it]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from pyhdf.SD import SD, SDC\n",
        "\n",
        "input_dir = \"/content/drive/MyDrive/MOD09GA_061-20250524_135742\"\n",
        "output_hr_dir = \"/content/drive/MyDrive/SRGANN_data/HR\"\n",
        "output_lr_dir = \"/content/drive/MyDrive/SRGANN_data/LR\"\n",
        "\n",
        "os.makedirs(output_hr_dir, exist_ok=True)\n",
        "os.makedirs(output_lr_dir, exist_ok=True)\n",
        "\n",
        "def process_modis_file(hdf_path, prefix):\n",
        "    try:\n",
        "        hdf = SD(hdf_path, SDC.READ)\n",
        "        raw_red = hdf.select('sur_refl_b01_1')[:]\n",
        "        red = np.where(raw_red == -28672, np.nan, raw_red).astype(np.float32)\n",
        "        red = red * 0.0001\n",
        "\n",
        "        # ✅ Apply percentile stretching (2nd to 98th)\n",
        "        p2 = np.nanpercentile(red, 2)\n",
        "        p98 = np.nanpercentile(red, 98)\n",
        "        red = np.clip((red - p2) / (p98 - p2), 0, 1)\n",
        "        red = np.nan_to_num(red, nan=0.0)\n",
        "\n",
        "        red_img = (red * 255).astype(np.uint8)\n",
        "        h, w = red_img.shape\n",
        "        crop = red_img[h//2 - 512:h//2 + 512, w//2 - 512:w//2 + 512]\n",
        "\n",
        "        pid = 0\n",
        "        for i in range(0, 1024, 256):\n",
        "            for j in range(0, 1024, 256):\n",
        "                patch = crop[i:i+256, j:j+256]\n",
        "                if patch.shape == (256, 256) and patch.mean() > 10:\n",
        "                    hr_img = Image.fromarray(patch)\n",
        "                    lr_img = hr_img.resize((64, 64), Image.BICUBIC)\n",
        "\n",
        "                    hr_path = os.path.join(output_hr_dir, f\"{prefix}_{pid}_HR.png\")\n",
        "                    lr_path = os.path.join(output_lr_dir, f\"{prefix}_{pid}_LR.png\")\n",
        "\n",
        "                    hr_img.save(hr_path)\n",
        "                    lr_img.save(lr_path)\n",
        "                    pid += 1\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error processing {hdf_path}: {e}\")\n",
        "\n",
        "file_list = [f for f in os.listdir(input_dir) if f.endswith('.hdf')]\n",
        "for idx, file in enumerate(tqdm(file_list, desc=\"🔄 Processing HDF files\")):\n",
        "    process_modis_file(os.path.join(input_dir, file), f\"img{idx}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S772QVkaH6x3"
      },
      "outputs": [],
      "source": [
        "# ✅ STEP 1: Import Libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import VGG19\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from glob import glob\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m88b6WS0H68G"
      },
      "outputs": [],
      "source": [
        "# ✅ STEP 2: Load Dataset (LR and HR images)\n",
        "def load_image_pair(lr_path, hr_path):\n",
        "    lr = load_img(lr_path, color_mode='grayscale', target_size=(64, 64))\n",
        "    hr = load_img(hr_path, color_mode='grayscale', target_size=(256, 256))\n",
        "    lr = img_to_array(lr) / 255.0\n",
        "    hr = img_to_array(hr) / 255.0\n",
        "    return lr, hr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sGFm9pfH7Eu"
      },
      "outputs": [],
      "source": [
        "def load_dataset(lr_dir, hr_dir):\n",
        "    lr_images = sorted(glob(os.path.join(lr_dir, '*.png')))\n",
        "    hr_images = sorted(glob(os.path.join(hr_dir, '*.png')))\n",
        "    dataset = [load_image_pair(lr, hr) for lr, hr in zip(lr_images, hr_images)]\n",
        "    return zip(*dataset)  # returns (LRs, HRs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJCCB4Z0H7Lw"
      },
      "outputs": [],
      "source": [
        "# ✅ STEP 3: Define the Generator\n",
        "\n",
        "def residual_block(x):\n",
        "    res = layers.Conv2D(64, 3, padding='same')(x)\n",
        "    res = layers.BatchNormalization()(res)\n",
        "    res = layers.PReLU(shared_axes=[1, 2])(res)\n",
        "    res = layers.Conv2D(64, 3, padding='same')(res)\n",
        "    res = layers.BatchNormalization()(res)\n",
        "    return layers.add([x, res])\n",
        "\n",
        "def build_generator():\n",
        "    inputs = layers.Input(shape=(64, 64, 1))\n",
        "    x = layers.Conv2D(64, 9, padding='same')(inputs)\n",
        "    x = layers.PReLU(shared_axes=[1, 2])(x)\n",
        "    res = x\n",
        "    for _ in range(16):\n",
        "        res = residual_block(res)\n",
        "    x = layers.Conv2D(64, 3, padding='same')(res)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.add([x, res])\n",
        "    x = layers.Conv2DTranspose(64, 3, strides=2, padding='same')(x)\n",
        "    x = layers.PReLU(shared_axes=[1, 2])(x)\n",
        "    x = layers.Conv2DTranspose(64, 3, strides=2, padding='same')(x)\n",
        "    x = layers.PReLU(shared_axes=[1, 2])(x)\n",
        "    output = layers.Conv2D(1, 9, activation='tanh', padding='same')(x)\n",
        "    return models.Model(inputs, output, name=\"generator\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDAIgodeH7RB"
      },
      "outputs": [],
      "source": [
        "# ✅ STEP 4: Define the Discriminator\n",
        "\n",
        "def build_discriminator():\n",
        "    def conv_block(x, filters, strides):\n",
        "        x = layers.Conv2D(filters, 3, strides=strides, padding='same')(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.LeakyReLU(0.2)(x)\n",
        "        return x\n",
        "\n",
        "    inputs = layers.Input(shape=(256, 256, 1))\n",
        "    x = layers.Conv2D(64, 3, strides=1, padding='same')(inputs)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "    for filters, strides in zip([64, 128, 128, 256, 256, 512, 512], [2, 1, 2, 1, 2, 1, 2]):\n",
        "        x = conv_block(x, filters, strides)\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(1024)(x)\n",
        "    x = layers.LeakyReLU(0.2)(x)\n",
        "    output = layers.Dense(1, activation='sigmoid')(x)\n",
        "    return models.Model(inputs, output, name=\"discriminator\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_jPw4wUH7Vg"
      },
      "outputs": [],
      "source": [
        "# ✅ STEP 5: VGG Feature Extractor (for perceptual loss)\n",
        "def build_vgg():\n",
        "    vgg = VGG19(include_top=False, weights='imagenet', input_shape=(256, 256, 3))\n",
        "    vgg.trainable = False\n",
        "    model = models.Model(inputs=vgg.input, outputs=vgg.get_layer('block5_conv4').output)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XoobSO7UH7ZP"
      },
      "outputs": [],
      "source": [
        "# ✅ STEP 6: Define Loss Functions\n",
        "binary_cross_entropy = tf.keras.losses.BinaryCrossentropy()\n",
        "mse = tf.keras.losses.MeanSquaredError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "noW-97Q5H7dP"
      },
      "outputs": [],
      "source": [
        "# ✅ STEP 7: PSNR Metric\n",
        "def psnr(y_true, y_pred):\n",
        "    return tf.image.psnr(y_true, y_pred, max_val=1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mj3c7_TKH7gx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc1f3d93-ab4f-4bc2-9ba7-9bbcb8cb39b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m80134624/80134624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "# ✅ STEP 8: Training Setup\n",
        "lr_images, hr_images = load_dataset('/content/drive/MyDrive/SRGANN_data/LR','/content/drive/MyDrive/SRGANN_data/HR')\n",
        "\n",
        "lr_images = np.array(lr_images).astype('float32')\n",
        "hr_images = np.array(hr_images).astype('float32')\n",
        "\n",
        "generator = build_generator()\n",
        "discriminator = build_discriminator()\n",
        "vgg = build_vgg()\n",
        "\n",
        "vgg.trainable = False\n",
        "\n",
        "# Optimizers\n",
        "g_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "d_optimizer = tf.keras.optimizers.Adam(1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pRK32dgRQWk"
      },
      "outputs": [],
      "source": [
        "# ✅ STEP 9: Training Loop\n",
        "@tf.function\n",
        "def train_step(lr, hr):\n",
        "    valid = tf.ones((lr.shape[0], 1))\n",
        "    fake = tf.zeros((lr.shape[0], 1))\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        fake_hr = generator(lr, training=True)\n",
        "        real_output = discriminator(hr, training=True)\n",
        "        fake_output = discriminator(fake_hr, training=True)\n",
        "\n",
        "        perceptual_loss = mse(vgg(tf.image.grayscale_to_rgb(hr)), vgg(tf.image.grayscale_to_rgb(fake_hr)))\n",
        "        content_loss = mse(hr, fake_hr)\n",
        "        adv_loss = binary_cross_entropy(valid, fake_output)\n",
        "        g_loss = 1e-3 * adv_loss + 0.006 * perceptual_loss + content_loss\n",
        "\n",
        "    grads = tape.gradient(g_loss, generator.trainable_variables)\n",
        "    g_optimizer.apply_gradients(zip(grads, generator.trainable_variables))\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        real_output = discriminator(hr, training=True)\n",
        "        fake_output = discriminator(generator(lr, training=False), training=True)\n",
        "        d_loss = binary_cross_entropy(valid, real_output) + binary_cross_entropy(fake, fake_output)\n",
        "\n",
        "    grads = tape.gradient(d_loss, discriminator.trainable_variables)\n",
        "    d_optimizer.apply_gradients(zip(grads, discriminator.trainable_variables))\n",
        "    return g_loss, d_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPEtwllTNOAq",
        "outputId": "583e5717-b3cb-401d-9f24-a21b55a8cb8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/10\n",
            "Batch 1/160 | G Loss: 0.1624, D Loss: 1.5094\n",
            "Batch 11/160 | G Loss: 0.0894, D Loss: 0.0007\n",
            "Batch 21/160 | G Loss: 0.0490, D Loss: 0.0001\n",
            "Batch 31/160 | G Loss: 0.0345, D Loss: 0.0000\n",
            "Batch 41/160 | G Loss: 0.0291, D Loss: 0.0000\n",
            "Batch 51/160 | G Loss: 0.0465, D Loss: 0.0011\n",
            "Batch 61/160 | G Loss: 0.0481, D Loss: 0.0000\n",
            "Batch 71/160 | G Loss: 0.0326, D Loss: 0.0000\n",
            "Batch 81/160 | G Loss: 0.0330, D Loss: 0.0000\n",
            "Batch 91/160 | G Loss: 0.0193, D Loss: 0.0000\n",
            "Batch 101/160 | G Loss: 0.0157, D Loss: 0.0000\n",
            "Batch 111/160 | G Loss: 0.0312, D Loss: 0.0000\n",
            "Batch 121/160 | G Loss: 0.0107, D Loss: 0.0000\n",
            "Batch 131/160 | G Loss: 0.0230, D Loss: 0.0000\n",
            "Batch 141/160 | G Loss: 0.0149, D Loss: 0.0000\n",
            "Batch 151/160 | G Loss: 0.0251, D Loss: 0.0000\n",
            "Batch 161/160 | G Loss: 0.0192, D Loss: 0.0000\n",
            "Epoch 1 complete → Generator loss: 6.8364, Discriminator loss: 107.7633\n",
            "\n",
            "Epoch 2/10\n",
            "Batch 1/160 | G Loss: 0.0545, D Loss: 0.0000\n",
            "Batch 11/160 | G Loss: 0.0104, D Loss: 0.0000\n",
            "Batch 21/160 | G Loss: 0.0123, D Loss: 0.0000\n",
            "Batch 31/160 | G Loss: 0.0119, D Loss: 0.0000\n",
            "Batch 41/160 | G Loss: 0.0173, D Loss: 0.0000\n",
            "Batch 51/160 | G Loss: 0.0144, D Loss: 0.0000\n",
            "Batch 61/160 | G Loss: 0.0165, D Loss: 0.0000\n",
            "Batch 71/160 | G Loss: 0.0162, D Loss: 0.0000\n",
            "Batch 81/160 | G Loss: 0.0118, D Loss: 0.0000\n",
            "Batch 91/160 | G Loss: 0.0138, D Loss: 2.5622\n",
            "Batch 101/160 | G Loss: 0.0127, D Loss: 12.2068\n",
            "Batch 111/160 | G Loss: 0.0233, D Loss: 0.0000\n",
            "Batch 121/160 | G Loss: 0.0075, D Loss: 1.5513\n",
            "Batch 131/160 | G Loss: 0.0131, D Loss: 0.0000\n",
            "Batch 141/160 | G Loss: 0.0079, D Loss: 0.0007\n",
            "Batch 151/160 | G Loss: 0.0164, D Loss: 0.0000\n",
            "Batch 161/160 | G Loss: 0.0165, D Loss: 0.0000\n",
            "Epoch 2 complete → Generator loss: 2.8366, Discriminator loss: 23.3657\n",
            "\n",
            "Epoch 3/10\n",
            "Batch 1/160 | G Loss: 0.0323, D Loss: 0.0000\n",
            "Batch 11/160 | G Loss: 0.0075, D Loss: 0.0000\n",
            "Batch 21/160 | G Loss: 0.0083, D Loss: 0.0000\n",
            "Batch 31/160 | G Loss: 0.0089, D Loss: 0.0000\n",
            "Batch 41/160 | G Loss: 0.0154, D Loss: 0.0001\n",
            "Batch 51/160 | G Loss: 0.0103, D Loss: 0.0000\n",
            "Batch 61/160 | G Loss: 0.0141, D Loss: 0.0000\n",
            "Batch 71/160 | G Loss: 0.0139, D Loss: 0.1814\n",
            "Batch 81/160 | G Loss: 0.0089, D Loss: 0.0000\n",
            "Batch 91/160 | G Loss: 0.0073, D Loss: 0.0000\n",
            "Batch 101/160 | G Loss: 0.0094, D Loss: 0.0000\n",
            "Batch 111/160 | G Loss: 0.0202, D Loss: 0.0000\n",
            "Batch 121/160 | G Loss: 0.0057, D Loss: 0.0000\n",
            "Batch 131/160 | G Loss: 0.0110, D Loss: 0.5879\n",
            "Batch 141/160 | G Loss: 0.0043, D Loss: 0.0000\n",
            "Batch 151/160 | G Loss: 0.0132, D Loss: 0.0000\n",
            "Batch 161/160 | G Loss: 0.0152, D Loss: 0.0000\n",
            "Epoch 3 complete → Generator loss: 2.1439, Discriminator loss: 58.5092\n",
            "\n",
            "Epoch 4/10\n",
            "Batch 1/160 | G Loss: 0.0211, D Loss: 0.0000\n",
            "Batch 11/160 | G Loss: 0.0070, D Loss: 0.0000\n",
            "Batch 21/160 | G Loss: 0.0119, D Loss: 0.0000\n",
            "Batch 31/160 | G Loss: 0.0124, D Loss: 0.3422\n",
            "Batch 41/160 | G Loss: 0.0196, D Loss: 0.0000\n",
            "Batch 51/160 | G Loss: 0.0108, D Loss: 0.0000\n",
            "Batch 61/160 | G Loss: 0.0140, D Loss: 0.0000\n",
            "Batch 71/160 | G Loss: 0.0144, D Loss: 2.2280\n",
            "Batch 81/160 | G Loss: 0.0091, D Loss: 1.0334\n",
            "Batch 91/160 | G Loss: 0.0075, D Loss: 0.0001\n",
            "Batch 101/160 | G Loss: 0.0089, D Loss: 8.6953\n",
            "Batch 111/160 | G Loss: 0.0204, D Loss: 0.0000\n",
            "Batch 121/160 | G Loss: 0.0078, D Loss: 21.6011\n",
            "Batch 131/160 | G Loss: 0.0138, D Loss: 9.9549\n",
            "Batch 141/160 | G Loss: 0.0053, D Loss: 7.1172\n",
            "Batch 151/160 | G Loss: 0.0162, D Loss: 13.8622\n",
            "Batch 161/160 | G Loss: 0.0155, D Loss: 0.0100\n",
            "Epoch 4 complete → Generator loss: 2.5554, Discriminator loss: 469.3920\n",
            "\n",
            "Epoch 5/10\n",
            "Batch 1/160 | G Loss: 0.0452, D Loss: 40.9921\n",
            "Batch 11/160 | G Loss: 0.0219, D Loss: 11.5238\n",
            "Batch 21/160 | G Loss: 0.0229, D Loss: 5.7644\n",
            "Batch 31/160 | G Loss: 0.0163, D Loss: 3.0412\n",
            "Batch 41/160 | G Loss: 0.0222, D Loss: 5.2780\n",
            "Batch 51/160 | G Loss: 0.0363, D Loss: 22.2841\n",
            "Batch 61/160 | G Loss: 0.0174, D Loss: 1.6112\n",
            "Batch 71/160 | G Loss: 0.0185, D Loss: 1.0088\n",
            "Batch 81/160 | G Loss: 0.0121, D Loss: 1.5643\n",
            "Batch 91/160 | G Loss: 0.0130, D Loss: 0.9730\n",
            "Batch 101/160 | G Loss: 0.0146, D Loss: 18.3070\n",
            "Batch 111/160 | G Loss: 0.0268, D Loss: 4.1758\n",
            "Batch 121/160 | G Loss: 0.0113, D Loss: 10.5220\n",
            "Batch 131/160 | G Loss: 0.0168, D Loss: 0.3373\n",
            "Batch 141/160 | G Loss: 0.0133, D Loss: 11.7286\n",
            "Batch 151/160 | G Loss: 0.0232, D Loss: 0.0000\n",
            "Batch 161/160 | G Loss: 0.0154, D Loss: 1.7029\n",
            "Epoch 5 complete → Generator loss: 3.4933, Discriminator loss: 974.1438\n",
            "\n",
            "Epoch 6/10\n",
            "Batch 1/160 | G Loss: 0.0318, D Loss: 0.0000\n",
            "Batch 11/160 | G Loss: 0.0073, D Loss: 2.0215\n",
            "Batch 21/160 | G Loss: 0.0160, D Loss: 1.0924\n",
            "Batch 31/160 | G Loss: 0.0151, D Loss: 4.0893\n",
            "Batch 41/160 | G Loss: 0.0442, D Loss: 4.9889\n",
            "Batch 51/160 | G Loss: 0.0459, D Loss: 19.4110\n",
            "Batch 61/160 | G Loss: 0.0204, D Loss: 0.1296\n",
            "Batch 71/160 | G Loss: 0.0182, D Loss: 0.9741\n",
            "Batch 81/160 | G Loss: 0.0131, D Loss: 2.7709\n",
            "Batch 91/160 | G Loss: 0.0243, D Loss: 0.2336\n",
            "Batch 101/160 | G Loss: 0.0114, D Loss: 0.0000\n",
            "Batch 111/160 | G Loss: 0.0219, D Loss: 0.5993\n",
            "Batch 121/160 | G Loss: 0.0150, D Loss: 0.0979\n",
            "Batch 131/160 | G Loss: 0.0290, D Loss: 0.0029\n",
            "Batch 141/160 | G Loss: 0.0123, D Loss: 0.0000\n",
            "Batch 151/160 | G Loss: 0.0182, D Loss: 0.0000\n",
            "Batch 161/160 | G Loss: 0.0190, D Loss: 0.0000\n",
            "Epoch 6 complete → Generator loss: 3.4276, Discriminator loss: 470.3145\n",
            "\n",
            "Epoch 7/10\n",
            "Batch 1/160 | G Loss: 0.0214, D Loss: 0.0000\n",
            "Batch 11/160 | G Loss: 0.0149, D Loss: 0.0000\n",
            "Batch 21/160 | G Loss: 0.0106, D Loss: 0.0660\n",
            "Batch 31/160 | G Loss: 0.0139, D Loss: 0.0008\n",
            "Batch 41/160 | G Loss: 0.0130, D Loss: 0.0279\n",
            "Batch 51/160 | G Loss: 0.0114, D Loss: 0.0000\n",
            "Batch 61/160 | G Loss: 0.0114, D Loss: 4.5744\n",
            "Batch 71/160 | G Loss: 0.0161, D Loss: 2.2282\n",
            "Batch 81/160 | G Loss: 0.0142, D Loss: 0.8103\n",
            "Batch 91/160 | G Loss: 0.0149, D Loss: 3.2238\n",
            "Batch 101/160 | G Loss: 0.0087, D Loss: 0.0000\n",
            "Batch 111/160 | G Loss: 0.0390, D Loss: 1.9859\n",
            "Batch 121/160 | G Loss: 0.0098, D Loss: 6.6893\n",
            "Batch 131/160 | G Loss: 0.0526, D Loss: 0.0000\n",
            "Batch 141/160 | G Loss: 0.0102, D Loss: 8.4719\n",
            "Batch 151/160 | G Loss: 0.0113, D Loss: 0.0000\n",
            "Batch 161/160 | G Loss: 0.0174, D Loss: 0.0073\n",
            "Epoch 7 complete → Generator loss: 3.2663, Discriminator loss: 145.6027\n",
            "\n",
            "Epoch 8/10\n",
            "Batch 1/160 | G Loss: 0.0168, D Loss: 0.0000\n",
            "Batch 11/160 | G Loss: 0.0090, D Loss: 0.0000\n",
            "Batch 21/160 | G Loss: 0.0126, D Loss: 1.0947\n",
            "Batch 31/160 | G Loss: 0.0104, D Loss: 0.0032\n",
            "Batch 41/160 | G Loss: 0.0084, D Loss: 0.6795\n",
            "Batch 51/160 | G Loss: 0.0160, D Loss: 0.0000\n",
            "Batch 61/160 | G Loss: 0.0245, D Loss: 0.3983\n",
            "Batch 71/160 | G Loss: 0.0160, D Loss: 3.5197\n",
            "Batch 81/160 | G Loss: 0.0158, D Loss: 0.0001\n",
            "Batch 91/160 | G Loss: 0.0080, D Loss: 0.0000\n",
            "Batch 101/160 | G Loss: 0.0216, D Loss: 0.0000\n",
            "Batch 111/160 | G Loss: 0.0281, D Loss: 0.1804\n",
            "Batch 121/160 | G Loss: 0.0124, D Loss: 1.3059\n",
            "Batch 131/160 | G Loss: 0.0384, D Loss: 0.0155\n",
            "Batch 141/160 | G Loss: 0.0145, D Loss: 6.6514\n",
            "Batch 151/160 | G Loss: 0.0158, D Loss: 2.2364\n",
            "Batch 161/160 | G Loss: 0.0255, D Loss: 0.0000\n",
            "Epoch 8 complete → Generator loss: 3.1295, Discriminator loss: 291.2169\n",
            "\n",
            "Epoch 9/10\n",
            "Batch 1/160 | G Loss: 0.0222, D Loss: 0.0000\n",
            "Batch 11/160 | G Loss: 0.0183, D Loss: 0.0238\n",
            "Batch 21/160 | G Loss: 0.0102, D Loss: 1.6966\n",
            "Batch 31/160 | G Loss: 0.0122, D Loss: 0.0000\n",
            "Batch 41/160 | G Loss: 0.0158, D Loss: 3.1175\n",
            "Batch 51/160 | G Loss: 0.0133, D Loss: 0.0010\n",
            "Batch 61/160 | G Loss: 0.0105, D Loss: 1.5656\n",
            "Batch 71/160 | G Loss: 0.0125, D Loss: 4.3920\n",
            "Batch 81/160 | G Loss: 0.0098, D Loss: 3.7139\n",
            "Batch 91/160 | G Loss: 0.0111, D Loss: 0.3764\n",
            "Batch 101/160 | G Loss: 0.0086, D Loss: 0.0001\n",
            "Batch 111/160 | G Loss: 0.0150, D Loss: 0.0020\n",
            "Batch 121/160 | G Loss: 0.0065, D Loss: 0.9396\n",
            "Batch 131/160 | G Loss: 0.0161, D Loss: 3.6710\n",
            "Batch 141/160 | G Loss: 0.0172, D Loss: 0.0479\n",
            "Batch 151/160 | G Loss: 0.0168, D Loss: 6.3159\n",
            "Batch 161/160 | G Loss: 0.0177, D Loss: 7.3797\n",
            "Epoch 9 complete → Generator loss: 2.8531, Discriminator loss: 367.8210\n",
            "\n",
            "Epoch 10/10\n",
            "Batch 1/160 | G Loss: 0.0131, D Loss: 2.1646\n",
            "Batch 11/160 | G Loss: 0.0072, D Loss: 2.7618\n",
            "Batch 21/160 | G Loss: 0.0097, D Loss: 0.0250\n",
            "Batch 31/160 | G Loss: 0.0091, D Loss: 0.3923\n",
            "Batch 41/160 | G Loss: 0.0189, D Loss: 4.3819\n",
            "Batch 51/160 | G Loss: 0.0155, D Loss: 0.3189\n",
            "Batch 61/160 | G Loss: 0.0137, D Loss: 0.4162\n",
            "Batch 71/160 | G Loss: 0.0187, D Loss: 1.7032\n",
            "Batch 81/160 | G Loss: 0.0112, D Loss: 1.4546\n",
            "Batch 91/160 | G Loss: 0.0088, D Loss: 0.0060\n",
            "Batch 101/160 | G Loss: 0.0214, D Loss: 0.0001\n",
            "Batch 111/160 | G Loss: 0.0159, D Loss: 6.0092\n",
            "Batch 121/160 | G Loss: 0.0078, D Loss: 0.3709\n",
            "Batch 131/160 | G Loss: 0.0129, D Loss: 8.1091\n",
            "Batch 141/160 | G Loss: 0.0061, D Loss: 0.0000\n",
            "Batch 151/160 | G Loss: 0.0111, D Loss: 0.0000\n",
            "Batch 161/160 | G Loss: 0.0151, D Loss: 0.0000\n",
            "Epoch 10 complete → Generator loss: 2.5311, Discriminator loss: 246.1947\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 10\n",
        "BATCH_SIZE = 8\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
        "    g_loss_total, d_loss_total = 0, 0\n",
        "    for i in range(0, len(lr_images), BATCH_SIZE):\n",
        "        lr_batch = lr_images[i:i+BATCH_SIZE]\n",
        "        hr_batch = hr_images[i:i+BATCH_SIZE]\n",
        "        g_loss, d_loss = train_step(lr_batch, hr_batch)\n",
        "\n",
        "        g_loss_total += g_loss\n",
        "        d_loss_total += d_loss\n",
        "\n",
        "        # ✅ Add a batch log every 10 batches\n",
        "        if i // BATCH_SIZE % 10 == 0:\n",
        "            print(f\"Batch {i//BATCH_SIZE + 1}/{len(lr_images) // BATCH_SIZE} | \"\n",
        "                  f\"G Loss: {g_loss:.4f}, D Loss: {d_loss:.4f}\")\n",
        "\n",
        "    print(f\"Epoch {epoch+1} complete → Generator loss: {g_loss_total:.4f}, Discriminator loss: {d_loss_total:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWWVUFM1BeRz"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}